{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Best CLoRA for SDXL: Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the necessary libraries installed\n",
    "!pip install diffusers transformers accelerate safetensors pytorch-metric-learning matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST have a 'utils.py' file in the same directory as this notebook.\n",
    "# It should contain the 'AttentionStore' and 'register_attention_control' code\n",
    "# from the original CLoRA repository.\n",
    "\n",
    "# For demonstration, here's a minimal version of 'utils.py' you can create:\n",
    "utils_code = \"\"\"\n",
    "import torch\n",
    "\n",
    "class AttentionStore:\n",
    "    def __init__(self, res, min_res=16):\n",
    "        self.res = res\n",
    "        self.min_res = min_res\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.attention_maps = {}\n",
    "\n",
    "    def B_transform(self, a, b):\n",
    "        return torch.einsum('b i j, b j d -> b i d', a, b)\n",
    "\n",
    "    def __call__(self, attn, is_cross, place_in_unet):\n",
    "        if is_cross:\n",
    "            if attn.shape[1] == (self.res[0] * self.res[1]):\n",
    "                self.attention_maps[place_in_unet] = attn\n",
    "            \n",
    "    def aggregate_attention(self, places_in_unet=('down', 'mid', 'up')):\n",
    "        att_map = [self.attention_maps[place] for place in places_in_unet if place in self.attention_maps]\n",
    "        att_map = torch.cat(att_map, dim=1)\n",
    "        return att_map\n",
    "\n",
    "def register_attention_control(model, controller):\n",
    "\n",
    "    def ca_forward(self, attn, is_cross, place_in_unet):\n",
    "        def forward(hidden_states, encoder_hidden_states=None, attention_mask=None, **cross_attention_kwargs):\n",
    "            \n",
    "            # The `cross_attention_kwargs` should be empty here since we are processing the input\n",
    "            # there are no kwargs to forward passed along and we don't want to break the code\n",
    "            # without passing along the kwargs. That is why we need to pop them from the code.\n",
    "            \n",
    "            if cross_attention_kwargs is not None and len(cross_attention_kwargs) > 0:\n",
    "                cross_attention_kwargs = cross_attention_kwargs.copy()\n",
    "                lora_scale = cross_attention_kwargs.pop('scale', 1.0)\n",
    "            else:\n",
    "                lora_scale = 1.0\n",
    "\n",
    "            if self.group_norm is not None:\n",
    "                hidden_states = self.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "            query = self.to_q(hidden_states)\n",
    "            query = self.head_to_batch_dim(query)\n",
    "\n",
    "            if encoder_hidden_states is None:\n",
    "                encoder_hidden_states = hidden_states\n",
    "            \n",
    "            key = self.to_k(encoder_hidden_states)\n",
    "            value = self.to_v(encoder_hidden_states)\n",
    "            \n",
    "            key = self.head_to_batch_dim(key)\n",
    "            value = self.head_to_batch_dim(value)\n",
    "\n",
    "            attention_probs = self.get_attention_scores(query, key, attention_mask)\n",
    "            \n",
    "            controller(attention_probs, is_cross, place_in_unet)\n",
    "            \n",
    "            hidden_states = torch.bmm(attention_probs, value)\n",
    "            hidden_states = self.batch_to_head_dim(hidden_states)\n",
    "\n",
    "            # linear proj\n",
    "            hidden_states = self.to_out[0](hidden_states)\n",
    "            # dropout\n",
    "            hidden_states = self.to_out[1](hidden_states)\n",
    "\n",
    "            return hidden_states\n",
    "        return forward\n",
    "\n",
    "    def register_recr(net_, count, place_in_unet):\n",
    "        if net_.__class__.__name__ == 'Attention':\n",
    "            net_.forward = ca_forward(net_, True, place_in_unet)\n",
    "            return count + 1\n",
    "        elif hasattr(net_, 'children'):\n",
    "            for net__ in net_.children():\n",
    "                count = register_recr(net__, count, place_in_unet)\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    sub_nets = model.named_children()\n",
    "    for net in sub_nets:\n",
    "        if \"down\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"down\")\n",
    "        elif \"up\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"up\")\n",
    "        elif \"mid\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"mid\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"utils.py\", \"w\") as f:\n",
    "    f.write(utils_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pipeline_clora_xl import CloraXLPipeline\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "# The custom pipeline inherits from the official SDXL pipeline,\n",
    "# so we can load it directly with .from_pretrained\n",
    "pipeline = CloraXLPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Recommended: Use the fixed VAE for better quality\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipeline.vae = vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SDXL-Compatible LoRAs\n",
    "**IMPORTANT**: You MUST use LoRAs trained specifically for SDXL. LoRAs for SD 1.5 will not work.\n",
    "You will need to create a `./models` directory and place your `.safetensors` files inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory for models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# NOTE: You must download SDXL LoRAs and place them here.\n",
    "# For example, from Civitai or Hugging Face Hub.\n",
    "# This code will fail if the files don't exist.\n",
    "lora_dog_path = \"./models/dog_xl.safetensors\"\n",
    "lora_cat_path = \"./models/cat_xl.safetensors\"\n",
    "\n",
    "try:\n",
    "    pipeline.load_lora_weights(lora_dog_path, adapter_name=\"dog\")\n",
    "    pipeline.load_lora_weights(lora_cat_path, adapter_name=\"cat\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load LoRAs. Please download them and place them in the 'models' directory.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Prompts and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CLoRA Setup --\n",
    "# LoRA for the background (empty string), then one for each concept\n",
    "fg_loras = [\"\", \"dog\", \"cat\"]\n",
    "\n",
    "# Prompt for the full scene, then one for each concept isolated with a trigger word (e.g., 'sks')\n",
    "fg_prompts = [\n",
    "    \"A photo of a cat and a dog in a garden, cinematic lighting\",\n",
    "    \"A photo of a sks dog in a garden, cinematic lighting\",\n",
    "    \"A photo of a sks cat in a garden, cinematic lighting\",\n",
    "]\n",
    "\n",
    "fg_negative = [\"blurry, low quality, cartoon, anime\"] * 3\n",
    "\n",
    "# -- Token Indices for Loss and Masking --\n",
    "# IMPORTANT: Re-calculate these for your specific prompts!\n",
    "print(\"--- Tokenization for Prompts ---\")\n",
    "for prompt in fg_prompts:\n",
    "    ids = pipeline.tokenizer(prompt).input_ids\n",
    "    tokens = pipeline.tokenizer.convert_ids_to_tokens(ids)\n",
    "    print({j: tok for j, tok in enumerate(tokens)})\n",
    "\n",
    "# Example Indices (MUST BE UPDATED BASED ON ABOVE OUTPUT)\n",
    "# Group tokens for the same concept together.\n",
    "important_token_indices = [\n",
    "    [[4], [7], [4]], # Concept 1: Cat\n",
    "    [[7], [4], [7]]  # Concept 2: Dog\n",
    "]\n",
    "\n",
    "# Which tokens' attention maps to use for creating the spatial masks\n",
    "mask_indices = [\n",
    "    [],       # Background - uses what's left over\n",
    "    [4],   # Dog mask from prompt 2 ('sks dog')\n",
    "    [4]    # Cat mask from prompt 3 ('sks cat')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "result = pipeline(\n",
    "    prompt_list=fg_prompts,\n",
    "    lora_list=fg_loras,\n",
    "    negative_prompt_list=fg_negative,\n",
    "    important_token_indices=important_token_indices,\n",
    "    mask_indices=mask_indices,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=40,\n",
    "    guidance_scale=8.0,\n",
    "    latent_update=True,\n",
    "    max_iter_to_alter=25,\n",
    "    step_size=0.02,\n",
    "    mask_threshold_alpha=0.4,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "image = result.images[0]\n",
    "image.show() # Use .show() to display in a separate window or just 'image' to display inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
