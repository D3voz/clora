{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# The Best CLoRA for SDXL: Usage"
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install diffusers transformers accelerate safetensors pytorch-metric-learning\n",
    "# Ensure you have a 'utils.py' with AttentionStore from the original CLoRA repo in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Model Initialization"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from pipeline_clora_xl import CloraXLPipeline\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "# The custom pipeline inherits from the official SDXL pipeline,\n",
    "# so we can load it directly with .from_pretrained\n",
    "pipeline = CloraXLPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Recommended: Use the fixed VAE for better quality\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipeline.vae = vae"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Load SDXL-Compatible LoRAs\n",
    "**IMPORTANT**: You MUST use LoRAs trained specifically for SDXL. LoRAs for SD 1.5 will not work."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Replace with actual paths to your downloaded or trained SDXL LoRAs\n",
    "lora_dog_path = \"./models/dog_xl.safetensors\"\n",
    "lora_cat_path = \"./models/cat_xl.safetensors\"\n",
    "\n",
    "pipeline.load_lora_weights(lora_dog_path, adapter_name=\"dog\")\n",
    "pipeline.load_lora_weights(lora_cat_path, adapter_name=\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Define Prompts and Hyperparameters"
  },
  {
   "cell_type": "code",
   "source": [
    "# -- CLoRA Setup --\n",
    "# LoRA for the background (empty string), then one for each concept\n",
    "fg_loras = [\"\", \"dog\", \"cat\"]\n",
    "\n",
    "# Prompt for the full scene, then one for each concept isolated with a trigger word (e.g., 'sks')\n",
    "fg_prompts = [\n",
    "    \"A photo of a cat and a dog in a garden, cinematic lighting\",\n",
    "    \"A photo of a sks dog in a garden, cinematic lighting\",\n",
    "    \"A photo of a sks cat in a garden, cinematic lighting\",\n",
    "]\n",
    "\n",
    "fg_negative = [\"blurry, low quality, cartoon, anime\"] * 3\n",
    "\n",
    "# -- Token Indices for Loss and Masking --\n",
    "# IMPORTANT: Re-calculate these for your specific prompts!\n",
    "for prompt in fg_prompts:\n",
    "    ids = pipeline.tokenizer(prompt).input_ids\n",
    "    tokens = pipeline.tokenizer.convert_ids_to_tokens(ids)\n",
    "    print({j: tok for j, tok in enumerate(tokens)})\n",
    "\n",
    "# Example Indices (MUST BE UPDATED BASED ON ABOVE OUTPUT)\n",
    "# Group tokens for the same concept together.\n",
    "important_token_indices = [\n",
    "    [[2, 3], [5], [2, 3]], # Concept 1: Cat (indices for 'a cat', 'cat', 'sks cat')\n",
    "    [[5], [2, 3], [5]]  # Concept 2: Dog (indices for 'a dog', 'sks dog', 'dog')\n",
    "]\n",
    "\n",
    "# Which tokens' attention maps to use for creating the spatial masks\n",
    "mask_indices = [\n",
    "    [],       # Background - uses what's left over\n",
    "    [2, 3],   # Dog mask from prompt 2 ('sks dog')\n",
    "    [2, 3]    # Cat mask from prompt 3 ('sks cat')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Run the Pipeline"
  },
  {
   "cell_type": "code",
   "source": [
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "result = pipeline(\n",
    "    prompt_list=fg_prompts,\n",
    "    lora_list=fg_loras,\n",
    "    negative_prompt_list=fg_negative,\n",
    "    important_token_indices=important_token_indices,\n",
    "    mask_indices=mask_indices,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=40,\n",
    "    guidance_scale=8.0,\n",
    "    latent_update=True,\n",
    "    max_iter_to_alter=25,\n",
    "    step_size=0.02, # Corresponds to latent update step size\n",
    "    mask_threshold_alpha=0.4,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "image = result.images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
