{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def attn_map_to_image(attn_image):\n",
    "    attn_image = (attn_image - attn_image.min()) / (attn_image.max() - attn_image.min())\n",
    "    cmap = plt.get_cmap(\"jet\")\n",
    "    attn_image = cmap(attn_image)\n",
    "    attn_image = np.delete(attn_image, 3, 2)\n",
    "    attn_image = attn_image * 255\n",
    "    attn_image = attn_image.astype(np.uint8)\n",
    "    attn_image = np.array(Image.fromarray(attn_image).resize((256, 256)).convert(\"RGB\"))\n",
    "\n",
    "    return attn_image\n",
    "\n",
    "\n",
    "def plot_attention_maps(pipeline, attention_maps, prompts, output_dir, step=None):\n",
    "    for i, (prompt, attention_map) in enumerate(zip(prompts, attention_maps)):\n",
    "        if step is not None:\n",
    "            output_path = output_dir / f\"step_{step:06d}_prompt_{i:06d}.png\"\n",
    "        else:\n",
    "            output_path = output_dir / f\"average_prompt_{i:06d}.png\"\n",
    "\n",
    "        output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        if step:\n",
    "            title = f\"Attention Maps `{prompt}` at Step {step}\"\n",
    "        else:\n",
    "            title = f\"Average Attention Maps `{prompt}`\"\n",
    "\n",
    "        ids = pipeline.tokenizer(prompt).input_ids\n",
    "        indices = {\n",
    "            j: tok\n",
    "            for tok, j in zip(\n",
    "                pipeline.tokenizer.convert_ids_to_tokens(ids), range(len(ids))\n",
    "            )\n",
    "        }\n",
    "\n",
    "        n_rows = math.ceil(math.sqrt(len(indices) - 2))\n",
    "        n_cols = math.ceil((len(indices) - 2) / n_rows)\n",
    "\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "        if n_rows == 1 or n_cols == 1:\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            [a.axis(\"off\") for a in ax.ravel()]\n",
    "        for idx, token in indices.items():\n",
    "            if token in [\"<|startoftext|>\", \"<|endoftext|>\"]:\n",
    "                continue\n",
    "            token = token.replace(\"</w>\", \"\")\n",
    "            col = (idx - 1) % n_cols\n",
    "            row = (idx - 1) // n_cols\n",
    "            attn_image = attention_map[:, :, idx].cpu().numpy()\n",
    "            attn_image = attn_map_to_image(attn_image)\n",
    "\n",
    "            if n_rows == 1 and n_cols == 1:\n",
    "                ax.imshow(attn_image, cmap=\"jet\")\n",
    "                ax.set_title(token)\n",
    "            elif n_rows == 1 or n_cols == 1:\n",
    "                ax[col].imshow(attn_image, cmap=\"jet\")\n",
    "                ax[col].set_title(token)\n",
    "            else:\n",
    "                ax[row, col].imshow(attn_image, cmap=\"jet\")\n",
    "                ax[row, col].set_title(token)\n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/tunahansmeral/Repos/CLoRA-archive/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  7.85it/s]\n",
      "/home/grads/tunahansmeral/Repos/CLoRA-archive/.venv/lib/python3.13/site-packages/diffusers/loaders/lora_pipeline.py:6134: FutureWarning: `LoraLoaderMixin` is deprecated and will be removed in version 1.0.0. LoraLoaderMixin is deprecated and this will be removed in a future version. Please use `StableDiffusionLoraLoaderMixin`, instead.\n",
      "  deprecate(\"LoraLoaderMixin\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "import sys \n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "from pipeline_clora import CloraPipeline\n",
    "\n",
    "pipeline = CloraPipeline.from_pretrained(\n",
    "    \"SG161222/Realistic_Vision_V5.1_noVAE\", # \"runwayml/stable-diffusion-v1-5\"\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipeline.vae = vae\n",
    "\n",
    "schedule_config = dict(pipeline.scheduler.config)\n",
    "pipeline.scheduler = DDIMScheduler.from_config(schedule_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_paths = {\n",
    "    \"dog\": \"models/dog/pytorch_lora_weights.safetensors\",\n",
    "    \"cat\": \"models/cat/pytorch_lora_weights.safetensors\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LoRA Weights to SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/tunahansmeral/Repos/CLoRA-archive/.venv/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for lora, lora_path in lora_paths.items():\n",
    "    pipeline.load_lora_weights(\n",
    "        lora_path,\n",
    "        adapter_name=lora,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 512 # Height\n",
    "W = 512 # Width\n",
    "seed = 53 # Seed\n",
    "num_inference_steps = 100 # Inference steps\n",
    "guidance_scale = 10.0 # Guidance scale\n",
    "steps_to_save_attention = [] # Which steps to save attention maps\n",
    "step_size = 20 # Step size for optimization\n",
    "max_iter_to_alter = 50 # Maximum iterations to alter the the latent\n",
    "iterative_steps = [0, 10, 20] # Which steps to apply iterative refinement\n",
    "iterative_step_steps = 20 # Iterative refinement steps\n",
    "latent_update = True # Update the latent\n",
    "apply_mask_after = 0 # Apply mask after this step\n",
    "attn_res = None # Attention resolution\n",
    "mask_threshold_alpha = 0.4 # Mask threshold alpha\n",
    "mask_erode = False # Mask erode\n",
    "mask_dilate = False # Mask dilate\n",
    "mask_opening = False # Mask opening\n",
    "mask_closing = False # Mask closing\n",
    "guidance_rescale = 0.0 # Guidance rescale\n",
    "clip_skip = None # Clip skip\n",
    "kwargs = {\"scale\": 1.0} # LoRA Scale\n",
    "use_text_encoder_lora = True # Use LoRAs' text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_lora = \"\"\n",
    "style_lora_weight = 0.8\n",
    "\n",
    "fg_loras = [\"\", \"dog\", \"cat\"]\n",
    "fg_prompts = [\n",
    "    \"A cat and a dog\",\n",
    "    \"A sks dog and a cat\",\n",
    "    \"A sks cat and a dog\",\n",
    "]\n",
    "fg_negative = [\n",
    "    \"nsfw\",\n",
    "    \"nsfw\",\n",
    "    \"nsfw\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<|startoftext|>', 1: 'a</w>', 2: 'cat</w>', 3: 'and</w>', 4: 'a</w>', 5: 'dog</w>', 6: '<|endoftext|>'}\n",
      "{0: '<|startoftext|>', 1: 'a</w>', 2: 'sks</w>', 3: 'dog</w>', 4: 'and</w>', 5: 'a</w>', 6: 'cat</w>', 7: '<|endoftext|>'}\n",
      "{0: '<|startoftext|>', 1: 'a</w>', 2: 'sks</w>', 3: 'cat</w>', 4: 'and</w>', 5: 'a</w>', 6: 'dog</w>', 7: '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "for prompt in fg_prompts:\n",
    "    ids = pipeline.tokenizer(prompt).input_ids\n",
    "    indices = {\n",
    "        j: tok\n",
    "        for tok, j in zip(\n",
    "            pipeline.tokenizer.convert_ids_to_tokens(ids), range(len(ids))\n",
    "        )\n",
    "    }\n",
    "    print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Indicate which tokens are important for each prompt\n",
    "important_token_indices = [\n",
    "    [\n",
    "        [2],    # Cat in Prompt 1\n",
    "        [5],    # Cat in Prompt 2\n",
    "        [2, 3]  # Cat in Prompt 3\n",
    "    ],\n",
    "    [\n",
    "        [6],    # Dog in Prompt 1\n",
    "        [2,3],  # Dog in Prompt 2\n",
    "        [6]     # Dog in Prompt 3\n",
    "    ],\n",
    "]\n",
    "\n",
    "mask_indices = [\n",
    "    [], # Backgroung mask indices from Prompt 1\n",
    "    [2, 3], # Dog mask indices from Prompt 2\n",
    "    [2, 3], # Cat mask indices from Prompt 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'unet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m image, attention_maps_to_save, masks_to_save = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfg_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfg_negative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfg_loras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_lora\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstyle_lora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstyle_lora_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstyle_lora_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimportant_token_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimportant_token_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_to_save_attention\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_to_save_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter_to_alter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter_to_alter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterative_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterative_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterative_steps_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterative_step_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_update\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatent_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_mask_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapply_mask_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_erode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_erode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_dilate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_dilate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_opening\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_opening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_closing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_closing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_threshold_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_threshold_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_rescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_text_encoder_lora\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_text_encoder_lora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m image[\u001b[32m0\u001b[39m].show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/CLoRA-archive/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/CLoRA-archive/pipeline_clora.py:1168\u001b[39m, in \u001b[36mCloraPipeline.__call__\u001b[39m\u001b[34m(self, prompt_list, negative_prompt_list, lora_list, style_lora, style_lora_weight, important_token_indices, mask_indices, steps_to_save_attention, step_size, max_iter_to_alter, iterative_steps, iterative_steps_steps, latent_update, apply_mask_after, mask_erode, mask_dilate, mask_opening, mask_closing, mask_threshold_alpha, height, width, num_inference_steps, guidance_scale, generator, cross_attention_kwargs, guidance_rescale, clip_skip, use_text_encoder_lora, **kwargs)\u001b[39m\n\u001b[32m   1165\u001b[39m     lora_names.append(style_lora)\n\u001b[32m   1166\u001b[39m     lora_weights.append(style_lora_weight)\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m prompt_embeds, negative_prompt_embeds = \u001b[38;5;28mself\u001b[39m.encode_prompt(\n\u001b[32m   1170\u001b[39m     prompt,\n\u001b[32m   1171\u001b[39m     device,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1176\u001b[39m     clip_skip=\u001b[38;5;28mself\u001b[39m.clip_skip,\n\u001b[32m   1177\u001b[39m )\n\u001b[32m   1178\u001b[39m prompt_embeds_list.append(\n\u001b[32m   1179\u001b[39m     torch.cat([negative_prompt_embeds, prompt_embeds])\n\u001b[32m   1180\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/CLoRA-archive/.venv/lib/python3.13/site-packages/diffusers/loaders/lora_base.py:762\u001b[39m, in \u001b[36mLoraBaseMixin.set_adapters\u001b[39m\u001b[34m(self, adapter_names, adapter_weights)\u001b[39m\n\u001b[32m    759\u001b[39m     _component_adapter_weights[component].append(component_adapter_weights)\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(model.\u001b[34m__class__\u001b[39m, ModelMixin):\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m     model.set_adapters(adapter_names, \u001b[43m_component_adapter_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(model.\u001b[34m__class__\u001b[39m, PreTrainedModel):\n\u001b[32m    764\u001b[39m     set_adapters_for_text_encoder(adapter_names, model, _component_adapter_weights[component])\n",
      "\u001b[31mKeyError\u001b[39m: 'unet'"
     ]
    }
   ],
   "source": [
    "image, attention_maps_to_save, masks_to_save = pipeline(\n",
    "    prompt_list=fg_prompts,\n",
    "    negative_prompt_list=fg_negative,\n",
    "    lora_list=fg_loras,\n",
    "    style_lora=style_lora,\n",
    "    style_lora_weight=style_lora_weight,\n",
    "    important_token_indices=important_token_indices,\n",
    "    mask_indices=mask_indices,\n",
    "    steps_to_save_attention=steps_to_save_attention,\n",
    "    step_size=step_size,\n",
    "    max_iter_to_alter=max_iter_to_alter,\n",
    "    iterative_steps=iterative_steps,\n",
    "    iterative_steps_steps=iterative_step_steps,\n",
    "    latent_update=latent_update,\n",
    "    apply_mask_after=apply_mask_after,\n",
    "    mask_erode=mask_erode,\n",
    "    mask_dilate=mask_dilate,\n",
    "    mask_opening=mask_opening,\n",
    "    mask_closing=mask_closing,\n",
    "    mask_threshold_alpha=mask_threshold_alpha,\n",
    "    height=H,\n",
    "    width=W,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=torch.Generator(device=pipeline.device).manual_seed(seed),\n",
    "    cross_attention_kwargs=kwargs,\n",
    "    guidance_rescale=guidance_rescale,\n",
    "    clip_skip=clip_skip,\n",
    "    use_text_encoder_lora=use_text_encoder_lora,\n",
    ")\n",
    "\n",
    "image[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
